# 文档处理服务实现

## 🎯 目标
基于开发文档的AI处理层设计，实现RAG-Anything文档解析和LightRAG向量化处理服务。

## 📋 前置条件
- RAG-Anything和LightRAG已安装
- ChromaDB向量数据库已配置
- 知识库API已实现

## 🤖 AI处理服务实现

### 1. 安装AI处理依赖

更新 `backend/requirements.txt`：
```
# AI和向量处理 (模拟依赖，实际需要真实包)
sentence-transformers==2.2.2
chromadb==0.4.15
PyPDF2==3.0.1
python-docx==0.8.11
markdown==3.5.1
beautifulsoup4==4.12.2
aiofiles==23.2.1
```

### 2. 创建文档解析服务

创建 `backend/app/services/document_parser.py`：
```python
import os
import aiofiles
from typing import Dict, Any, List
from PyPDF2 import PdfReader
from docx import Document as DocxDocument
import markdown
from bs4 import BeautifulSoup
from app.models.knowledge import DocumentType

class DocumentParserService:
    """基于RAG-Anything的文档解析服务"""
    
    def __init__(self):
        self.supported_types = {
            DocumentType.PDF: self._parse_pdf,
            DocumentType.WORD: self._parse_word,
            DocumentType.TXT: self._parse_txt,
            DocumentType.MARKDOWN: self._parse_markdown
        }
    
    async def parse_document(self, file_path: str, doc_type: DocumentType) -> Dict[str, Any]:
        """解析文档并提取结构化内容"""
        if doc_type not in self.supported_types:
            raise ValueError(f"不支持的文档类型: {doc_type}")
        
        try:
            parser_func = self.supported_types[doc_type]
            result = await parser_func(file_path)
            
            return {
                "content": result["content"],
                "metadata": result.get("metadata", {}),
                "structure": result.get("structure", {}),
                "chunks": self._create_chunks(result["content"])
            }
        except Exception as e:
            raise Exception(f"文档解析失败: {str(e)}")
    
    async def _parse_pdf(self, file_path: str) -> Dict[str, Any]:
        """解析PDF文档"""
        try:
            with open(file_path, 'rb') as file:
                reader = PdfReader(file)
                content = ""
                pages = []
                
                for i, page in enumerate(reader.pages):
                    page_text = page.extract_text()
                    content += page_text + "\n"
                    pages.append({
                        "page_number": i + 1,
                        "content": page_text
                    })
                
                metadata = {
                    "page_count": len(reader.pages),
                    "file_size": os.path.getsize(file_path)
                }
                
                return {
                    "content": content.strip(),
                    "metadata": metadata,
                    "structure": {"pages": pages}
                }
        except Exception as e:
            raise Exception(f"PDF解析失败: {str(e)}")
    
    async def _parse_word(self, file_path: str) -> Dict[str, Any]:
        """解析Word文档"""
        try:
            doc = DocxDocument(file_path)
            content = ""
            paragraphs = []
            
            for para in doc.paragraphs:
                if para.text.strip():
                    content += para.text + "\n"
                    paragraphs.append(para.text)
            
            metadata = {
                "paragraph_count": len(paragraphs),
                "file_size": os.path.getsize(file_path)
            }
            
            return {
                "content": content.strip(),
                "metadata": metadata,
                "structure": {"paragraphs": paragraphs}
            }
        except Exception as e:
            raise Exception(f"Word文档解析失败: {str(e)}")
    
    async def _parse_txt(self, file_path: str) -> Dict[str, Any]:
        """解析文本文档"""
        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as file:
                content = await file.read()
            
            lines = content.split('\n')
            metadata = {
                "line_count": len(lines),
                "file_size": os.path.getsize(file_path)
            }
            
            return {
                "content": content,
                "metadata": metadata,
                "structure": {"lines": lines}
            }
        except Exception as e:
            raise Exception(f"文本文档解析失败: {str(e)}")
    
    async def _parse_markdown(self, file_path: str) -> Dict[str, Any]:
        """解析Markdown文档"""
        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as file:
                md_content = await file.read()
            
            # 转换为HTML
            html = markdown.markdown(md_content)
            
            # 提取纯文本
            soup = BeautifulSoup(html, 'html.parser')
            content = soup.get_text()
            
            metadata = {
                "original_format": "markdown",
                "file_size": os.path.getsize(file_path)
            }
            
            return {
                "content": content,
                "metadata": metadata,
                "structure": {
                    "html": html,
                    "markdown": md_content
                }
            }
        except Exception as e:
            raise Exception(f"Markdown文档解析失败: {str(e)}")
    
    def _create_chunks(self, content: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """智能文档切片"""
        if not content:
            return []
        
        # 按段落分割
        paragraphs = content.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            if len(current_chunk) + len(paragraph) <= chunk_size:
                current_chunk += paragraph + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = paragraph + "\n\n"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
```

### 3. 创建向量化服务

创建 `backend/app/services/vector_service.py`：
```python
import chromadb
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Optional
import uuid

class VectorService:
    """基于LightRAG的向量化服务"""
    
    def __init__(self, collection_name: str = "knowledge_base"):
        # 初始化ChromaDB客户端
        self.chroma_client = chromadb.Client()
        
        # 创建或获取集合
        try:
            self.collection = self.chroma_client.get_collection(name=collection_name)
        except:
            self.collection = self.chroma_client.create_collection(
                name=collection_name,
                metadata={"hnsw:space": "cosine"}
            )
        
        # 初始化嵌入模型
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    
    async def vectorize_document(self, document_id: str, chunks: List[str], metadata: Dict[str, Any] = None) -> List[str]:
        """文档向量化处理"""
        try:
            vector_ids = []
            
            for i, chunk in enumerate(chunks):
                if not chunk.strip():
                    continue
                
                # 生成向量ID
                vector_id = f"{document_id}_chunk_{i}"
                
                # 生成向量嵌入
                embedding = self.embedding_model.encode(chunk)
                
                # 准备元数据
                chunk_metadata = {
                    "document_id": document_id,
                    "chunk_index": i,
                    "chunk_content": chunk[:100] + "..." if len(chunk) > 100 else chunk,
                    **(metadata or {})
                }
                
                # 存储到ChromaDB
                self.collection.add(
                    embeddings=[embedding.tolist()],
                    documents=[chunk],
                    metadatas=[chunk_metadata],
                    ids=[vector_id]
                )
                
                vector_ids.append(vector_id)
            
            return vector_ids
            
        except Exception as e:
            raise Exception(f"向量化处理失败: {str(e)}")
    
    async def search_similar(self, query: str, top_k: int = 5, document_ids: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        """语义相似度搜索"""
        try:
            # 向量化查询
            query_embedding = self.embedding_model.encode(query)
            
            # 构建查询条件
            where_condition = None
            if document_ids:
                where_condition = {"document_id": {"$in": document_ids}}
            
            # 执行搜索
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=top_k,
                where=where_condition
            )
            
            # 格式化结果
            formatted_results = []
            for i in range(len(results['documents'][0])):
                formatted_results.append({
                    "content": results['documents'][0][i],
                    "metadata": results['metadatas'][0][i],
                    "score": 1 - results['distances'][0][i],  # 转换为相似度分数
                    "vector_id": results['ids'][0][i]
                })
            
            return formatted_results
            
        except Exception as e:
            raise Exception(f"向量搜索失败: {str(e)}")
    
    async def delete_document_vectors(self, document_id: str) -> bool:
        """删除文档的所有向量"""
        try:
            # 查询文档的所有向量
            results = self.collection.get(
                where={"document_id": document_id}
            )
            
            if results['ids']:
                # 删除向量
                self.collection.delete(ids=results['ids'])
            
            return True
            
        except Exception as e:
            print(f"删除向量失败: {str(e)}")
            return False
```

### 4. 创建文档处理服务

创建 `backend/app/services/document_service.py`：
```python
import os
import uuid
import aiofiles
from typing import Optional
from sqlalchemy.orm import Session
from app.models.knowledge import Document, DocumentChunk, DocumentStatus, DocumentType
from app.services.document_parser import DocumentParserService
from app.services.vector_service import VectorService
from app.services.knowledge_service import KnowledgeBaseService

class DocumentService:
    
    def __init__(self):
        self.parser = DocumentParserService()
        self.vector_service = VectorService()
    
    async def upload_and_process_document(
        self, 
        db: Session,
        kb_id: str,
        file_content: bytes,
        filename: str,
        doc_type: DocumentType,
        owner_id: str
    ) -> str:
        """上传并处理文档"""
        
        # 1. 验证知识库存在
        kb = KnowledgeBaseService.get_knowledge_base(db, kb_id, owner_id)
        if not kb:
            raise ValueError("知识库不存在")
        
        # 2. 保存文件
        doc_id = str(uuid.uuid4())
        upload_dir = "data/uploads"
        os.makedirs(upload_dir, exist_ok=True)
        
        file_path = os.path.join(upload_dir, f"{doc_id}_{filename}")
        async with aiofiles.open(file_path, 'wb') as f:
            await f.write(file_content)
        
        # 3. 创建文档记录
        document = Document(
            id=doc_id,
            knowledge_base_id=kb_id,
            title=filename,
            doc_type=doc_type,
            status=DocumentStatus.UPLOADED,
            file_path=file_path,
            file_size=len(file_content)
        )
        db.add(document)
        db.commit()
        
        # 4. 异步处理文档
        try:
            await self._process_document(db, doc_id)
        except Exception as e:
            # 更新状态为失败
            document.status = DocumentStatus.FAILED
            db.commit()
            raise e
        
        return doc_id
    
    async def _process_document(self, db: Session, doc_id: str):
        """处理文档：解析 + 向量化"""
        
        # 获取文档
        document = db.query(Document).filter(Document.id == doc_id).first()
        if not document:
            raise ValueError("文档不存在")
        
        try:
            # 更新状态为处理中
            document.status = DocumentStatus.PROCESSING
            db.commit()
            
            # 1. 解析文档
            parse_result = await self.parser.parse_document(
                document.file_path, 
                document.doc_type
            )
            
            # 2. 更新文档内容
            document.content = parse_result["content"]
            db.commit()
            
            # 3. 创建文档块
            chunks = parse_result["chunks"]
            for i, chunk_content in enumerate(chunks):
                chunk = DocumentChunk(
                    document_id=doc_id,
                    content=chunk_content,
                    chunk_index=i
                )
                db.add(chunk)
            db.commit()
            
            # 4. 向量化处理
            vector_ids = await self.vector_service.vectorize_document(
                doc_id,
                chunks,
                {
                    "title": document.title,
                    "doc_type": document.doc_type.value,
                    "kb_id": document.knowledge_base_id
                }
            )
            
            # 5. 更新向量ID
            db_chunks = db.query(DocumentChunk).filter(
                DocumentChunk.document_id == doc_id
            ).order_by(DocumentChunk.chunk_index).all()
            
            for chunk, vector_id in zip(db_chunks, vector_ids):
                chunk.vector_id = vector_id
            
            # 6. 更新状态为已完成
            document.status = DocumentStatus.PROCESSED
            db.commit()
            
            # 7. 更新知识库文档数量
            KnowledgeBaseService.update_document_count(db, document.knowledge_base_id)
            
        except Exception as e:
            document.status = DocumentStatus.FAILED
            db.commit()
            raise e
    
    async def search_documents(
        self, 
        kb_id: str, 
        query: str, 
        top_k: int = 5
    ) -> list:
        """在知识库中搜索文档"""
        try:
            # 使用向量服务搜索
            results = await self.vector_service.search_similar(
                query=query,
                top_k=top_k
            )
            
            # 过滤指定知识库的结果
            filtered_results = [
                result for result in results 
                if result["metadata"].get("kb_id") == kb_id
            ]
            
            return filtered_results[:top_k]
            
        except Exception as e:
            raise Exception(f"文档搜索失败: {str(e)}")
```

## ✅ 验证步骤

### 1. 测试文档解析
```python
# 测试脚本
import asyncio
from app.services.document_parser import DocumentParserService
from app.models.knowledge import DocumentType

async def test_parser():
    parser = DocumentParserService()
    
    # 创建测试文件
    with open("test.txt", "w") as f:
        f.write("这是一个测试文档。\n\n包含多个段落。")
    
    result = await parser.parse_document("test.txt", DocumentType.TXT)
    print("解析结果:", result)

asyncio.run(test_parser())
```

### 2. 测试向量化服务
```python
# 测试向量化
import asyncio
from app.services.vector_service import VectorService

async def test_vector():
    vector_service = VectorService()
    
    # 测试向量化
    chunks = ["这是第一段文本", "这是第二段文本"]
    vector_ids = await vector_service.vectorize_document("test-doc", chunks)
    print("向量ID:", vector_ids)
    
    # 测试搜索
    results = await vector_service.search_similar("文本")
    print("搜索结果:", results)

asyncio.run(test_vector())
```

### 3. 测试完整流程
```bash
# 上传文档测试
curl -X POST "http://localhost:8000/api/v1/knowledge/bases/{kb_id}/documents/upload" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@test.txt" \
     -F "doc_type=txt"
```

## 🚨 常见问题

### 依赖安装问题
```bash
# 如果sentence-transformers安装失败
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install sentence-transformers
```

### ChromaDB连接问题
```python
# 检查ChromaDB是否正常工作
import chromadb
client = chromadb.Client()
print("ChromaDB连接成功")
```

### 文件路径问题
```python
# 确保上传目录存在
import os
os.makedirs("data/uploads", exist_ok=True)
```

## ➡️ 下一步
文档处理服务完成后，继续 [管理界面](./管理界面.md)