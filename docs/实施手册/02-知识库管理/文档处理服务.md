# æ–‡æ¡£å¤„ç†æœåŠ¡å®ç°

## ğŸ¯ ç›®æ ‡
åŸºäºå¼€å‘æ–‡æ¡£çš„AIå¤„ç†å±‚è®¾è®¡ï¼Œå®ç°RAG-Anythingæ–‡æ¡£è§£æå’ŒLightRAGå‘é‡åŒ–å¤„ç†æœåŠ¡ã€‚

## ğŸ“‹ å‰ç½®æ¡ä»¶
- RAG-Anythingå’ŒLightRAGå·²å®‰è£…
- ChromaDBå‘é‡æ•°æ®åº“å·²é…ç½®
- çŸ¥è¯†åº“APIå·²å®ç°

## ğŸ¤– AIå¤„ç†æœåŠ¡å®ç°

### 1. å®‰è£…AIå¤„ç†ä¾èµ–

æ›´æ–° `backend/requirements.txt`ï¼š
```
# AIå’Œå‘é‡å¤„ç† (æ¨¡æ‹Ÿä¾èµ–ï¼Œå®é™…éœ€è¦çœŸå®åŒ…)
sentence-transformers==2.2.2
chromadb==0.4.15
PyPDF2==3.0.1
python-docx==0.8.11
markdown==3.5.1
beautifulsoup4==4.12.2
aiofiles==23.2.1
```

### 2. åˆ›å»ºæ–‡æ¡£è§£ææœåŠ¡

åˆ›å»º `backend/app/services/document_parser.py`ï¼š
```python
import os
import aiofiles
from typing import Dict, Any, List
from PyPDF2 import PdfReader
from docx import Document as DocxDocument
import markdown
from bs4 import BeautifulSoup
from app.models.knowledge import DocumentType

class DocumentParserService:
    """åŸºäºRAG-Anythingçš„æ–‡æ¡£è§£ææœåŠ¡"""
    
    def __init__(self):
        self.supported_types = {
            DocumentType.PDF: self._parse_pdf,
            DocumentType.WORD: self._parse_word,
            DocumentType.TXT: self._parse_txt,
            DocumentType.MARKDOWN: self._parse_markdown
        }
    
    async def parse_document(self, file_path: str, doc_type: DocumentType) -> Dict[str, Any]:
        """è§£ææ–‡æ¡£å¹¶æå–ç»“æ„åŒ–å†…å®¹"""
        if doc_type not in self.supported_types:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡æ¡£ç±»å‹: {doc_type}")
        
        try:
            parser_func = self.supported_types[doc_type]
            result = await parser_func(file_path)
            
            return {
                "content": result["content"],
                "metadata": result.get("metadata", {}),
                "structure": result.get("structure", {}),
                "chunks": self._create_chunks(result["content"])
            }
        except Exception as e:
            raise Exception(f"æ–‡æ¡£è§£æå¤±è´¥: {str(e)}")
    
    async def _parse_pdf(self, file_path: str) -> Dict[str, Any]:
        """è§£æPDFæ–‡æ¡£"""
        try:
            with open(file_path, 'rb') as file:
                reader = PdfReader(file)
                content = ""
                pages = []
                
                for i, page in enumerate(reader.pages):
                    page_text = page.extract_text()
                    content += page_text + "\n"
                    pages.append({
                        "page_number": i + 1,
                        "content": page_text
                    })
                
                metadata = {
                    "page_count": len(reader.pages),
                    "file_size": os.path.getsize(file_path)
                }
                
                return {
                    "content": content.strip(),
                    "metadata": metadata,
                    "structure": {"pages": pages}
                }
        except Exception as e:
            raise Exception(f"PDFè§£æå¤±è´¥: {str(e)}")
    
    async def _parse_word(self, file_path: str) -> Dict[str, Any]:
        """è§£æWordæ–‡æ¡£"""
        try:
            doc = DocxDocument(file_path)
            content = ""
            paragraphs = []
            
            for para in doc.paragraphs:
                if para.text.strip():
                    content += para.text + "\n"
                    paragraphs.append(para.text)
            
            metadata = {
                "paragraph_count": len(paragraphs),
                "file_size": os.path.getsize(file_path)
            }
            
            return {
                "content": content.strip(),
                "metadata": metadata,
                "structure": {"paragraphs": paragraphs}
            }
        except Exception as e:
            raise Exception(f"Wordæ–‡æ¡£è§£æå¤±è´¥: {str(e)}")
    
    async def _parse_txt(self, file_path: str) -> Dict[str, Any]:
        """è§£ææ–‡æœ¬æ–‡æ¡£"""
        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as file:
                content = await file.read()
            
            lines = content.split('\n')
            metadata = {
                "line_count": len(lines),
                "file_size": os.path.getsize(file_path)
            }
            
            return {
                "content": content,
                "metadata": metadata,
                "structure": {"lines": lines}
            }
        except Exception as e:
            raise Exception(f"æ–‡æœ¬æ–‡æ¡£è§£æå¤±è´¥: {str(e)}")
    
    async def _parse_markdown(self, file_path: str) -> Dict[str, Any]:
        """è§£æMarkdownæ–‡æ¡£"""
        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as file:
                md_content = await file.read()
            
            # è½¬æ¢ä¸ºHTML
            html = markdown.markdown(md_content)
            
            # æå–çº¯æ–‡æœ¬
            soup = BeautifulSoup(html, 'html.parser')
            content = soup.get_text()
            
            metadata = {
                "original_format": "markdown",
                "file_size": os.path.getsize(file_path)
            }
            
            return {
                "content": content,
                "metadata": metadata,
                "structure": {
                    "html": html,
                    "markdown": md_content
                }
            }
        except Exception as e:
            raise Exception(f"Markdownæ–‡æ¡£è§£æå¤±è´¥: {str(e)}")
    
    def _create_chunks(self, content: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """æ™ºèƒ½æ–‡æ¡£åˆ‡ç‰‡"""
        if not content:
            return []
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split('\n\n')
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            if len(current_chunk) + len(paragraph) <= chunk_size:
                current_chunk += paragraph + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = paragraph + "\n\n"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
```

### 3. åˆ›å»ºå‘é‡åŒ–æœåŠ¡

åˆ›å»º `backend/app/services/vector_service.py`ï¼š
```python
import chromadb
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Optional
import uuid

class VectorService:
    """åŸºäºLightRAGçš„å‘é‡åŒ–æœåŠ¡"""
    
    def __init__(self, collection_name: str = "knowledge_base"):
        # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯
        self.chroma_client = chromadb.Client()
        
        # åˆ›å»ºæˆ–è·å–é›†åˆ
        try:
            self.collection = self.chroma_client.get_collection(name=collection_name)
        except:
            self.collection = self.chroma_client.create_collection(
                name=collection_name,
                metadata={"hnsw:space": "cosine"}
            )
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    
    async def vectorize_document(self, document_id: str, chunks: List[str], metadata: Dict[str, Any] = None) -> List[str]:
        """æ–‡æ¡£å‘é‡åŒ–å¤„ç†"""
        try:
            vector_ids = []
            
            for i, chunk in enumerate(chunks):
                if not chunk.strip():
                    continue
                
                # ç”Ÿæˆå‘é‡ID
                vector_id = f"{document_id}_chunk_{i}"
                
                # ç”Ÿæˆå‘é‡åµŒå…¥
                embedding = self.embedding_model.encode(chunk)
                
                # å‡†å¤‡å…ƒæ•°æ®
                chunk_metadata = {
                    "document_id": document_id,
                    "chunk_index": i,
                    "chunk_content": chunk[:100] + "..." if len(chunk) > 100 else chunk,
                    **(metadata or {})
                }
                
                # å­˜å‚¨åˆ°ChromaDB
                self.collection.add(
                    embeddings=[embedding.tolist()],
                    documents=[chunk],
                    metadatas=[chunk_metadata],
                    ids=[vector_id]
                )
                
                vector_ids.append(vector_id)
            
            return vector_ids
            
        except Exception as e:
            raise Exception(f"å‘é‡åŒ–å¤„ç†å¤±è´¥: {str(e)}")
    
    async def search_similar(self, query: str, top_k: int = 5, document_ids: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        """è¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢"""
        try:
            # å‘é‡åŒ–æŸ¥è¯¢
            query_embedding = self.embedding_model.encode(query)
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_condition = None
            if document_ids:
                where_condition = {"document_id": {"$in": document_ids}}
            
            # æ‰§è¡Œæœç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=top_k,
                where=where_condition
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for i in range(len(results['documents'][0])):
                formatted_results.append({
                    "content": results['documents'][0][i],
                    "metadata": results['metadatas'][0][i],
                    "score": 1 - results['distances'][0][i],  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                    "vector_id": results['ids'][0][i]
                })
            
            return formatted_results
            
        except Exception as e:
            raise Exception(f"å‘é‡æœç´¢å¤±è´¥: {str(e)}")
    
    async def delete_document_vectors(self, document_id: str) -> bool:
        """åˆ é™¤æ–‡æ¡£çš„æ‰€æœ‰å‘é‡"""
        try:
            # æŸ¥è¯¢æ–‡æ¡£çš„æ‰€æœ‰å‘é‡
            results = self.collection.get(
                where={"document_id": document_id}
            )
            
            if results['ids']:
                # åˆ é™¤å‘é‡
                self.collection.delete(ids=results['ids'])
            
            return True
            
        except Exception as e:
            print(f"åˆ é™¤å‘é‡å¤±è´¥: {str(e)}")
            return False
```

### 4. åˆ›å»ºæ–‡æ¡£å¤„ç†æœåŠ¡

åˆ›å»º `backend/app/services/document_service.py`ï¼š
```python
import os
import uuid
import aiofiles
from typing import Optional
from sqlalchemy.orm import Session
from app.models.knowledge import Document, DocumentChunk, DocumentStatus, DocumentType
from app.services.document_parser import DocumentParserService
from app.services.vector_service import VectorService
from app.services.knowledge_service import KnowledgeBaseService

class DocumentService:
    
    def __init__(self):
        self.parser = DocumentParserService()
        self.vector_service = VectorService()
    
    async def upload_and_process_document(
        self, 
        db: Session,
        kb_id: str,
        file_content: bytes,
        filename: str,
        doc_type: DocumentType,
        owner_id: str
    ) -> str:
        """ä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£"""
        
        # 1. éªŒè¯çŸ¥è¯†åº“å­˜åœ¨
        kb = KnowledgeBaseService.get_knowledge_base(db, kb_id, owner_id)
        if not kb:
            raise ValueError("çŸ¥è¯†åº“ä¸å­˜åœ¨")
        
        # 2. ä¿å­˜æ–‡ä»¶
        doc_id = str(uuid.uuid4())
        upload_dir = "data/uploads"
        os.makedirs(upload_dir, exist_ok=True)
        
        file_path = os.path.join(upload_dir, f"{doc_id}_{filename}")
        async with aiofiles.open(file_path, 'wb') as f:
            await f.write(file_content)
        
        # 3. åˆ›å»ºæ–‡æ¡£è®°å½•
        document = Document(
            id=doc_id,
            knowledge_base_id=kb_id,
            title=filename,
            doc_type=doc_type,
            status=DocumentStatus.UPLOADED,
            file_path=file_path,
            file_size=len(file_content)
        )
        db.add(document)
        db.commit()
        
        # 4. å¼‚æ­¥å¤„ç†æ–‡æ¡£
        try:
            await self._process_document(db, doc_id)
        except Exception as e:
            # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            document.status = DocumentStatus.FAILED
            db.commit()
            raise e
        
        return doc_id
    
    async def _process_document(self, db: Session, doc_id: str):
        """å¤„ç†æ–‡æ¡£ï¼šè§£æ + å‘é‡åŒ–"""
        
        # è·å–æ–‡æ¡£
        document = db.query(Document).filter(Document.id == doc_id).first()
        if not document:
            raise ValueError("æ–‡æ¡£ä¸å­˜åœ¨")
        
        try:
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            document.status = DocumentStatus.PROCESSING
            db.commit()
            
            # 1. è§£ææ–‡æ¡£
            parse_result = await self.parser.parse_document(
                document.file_path, 
                document.doc_type
            )
            
            # 2. æ›´æ–°æ–‡æ¡£å†…å®¹
            document.content = parse_result["content"]
            db.commit()
            
            # 3. åˆ›å»ºæ–‡æ¡£å—
            chunks = parse_result["chunks"]
            for i, chunk_content in enumerate(chunks):
                chunk = DocumentChunk(
                    document_id=doc_id,
                    content=chunk_content,
                    chunk_index=i
                )
                db.add(chunk)
            db.commit()
            
            # 4. å‘é‡åŒ–å¤„ç†
            vector_ids = await self.vector_service.vectorize_document(
                doc_id,
                chunks,
                {
                    "title": document.title,
                    "doc_type": document.doc_type.value,
                    "kb_id": document.knowledge_base_id
                }
            )
            
            # 5. æ›´æ–°å‘é‡ID
            db_chunks = db.query(DocumentChunk).filter(
                DocumentChunk.document_id == doc_id
            ).order_by(DocumentChunk.chunk_index).all()
            
            for chunk, vector_id in zip(db_chunks, vector_ids):
                chunk.vector_id = vector_id
            
            # 6. æ›´æ–°çŠ¶æ€ä¸ºå·²å®Œæˆ
            document.status = DocumentStatus.PROCESSED
            db.commit()
            
            # 7. æ›´æ–°çŸ¥è¯†åº“æ–‡æ¡£æ•°é‡
            KnowledgeBaseService.update_document_count(db, document.knowledge_base_id)
            
        except Exception as e:
            document.status = DocumentStatus.FAILED
            db.commit()
            raise e
    
    async def search_documents(
        self, 
        kb_id: str, 
        query: str, 
        top_k: int = 5
    ) -> list:
        """åœ¨çŸ¥è¯†åº“ä¸­æœç´¢æ–‡æ¡£"""
        try:
            # ä½¿ç”¨å‘é‡æœåŠ¡æœç´¢
            results = await self.vector_service.search_similar(
                query=query,
                top_k=top_k
            )
            
            # è¿‡æ»¤æŒ‡å®šçŸ¥è¯†åº“çš„ç»“æœ
            filtered_results = [
                result for result in results 
                if result["metadata"].get("kb_id") == kb_id
            ]
            
            return filtered_results[:top_k]
            
        except Exception as e:
            raise Exception(f"æ–‡æ¡£æœç´¢å¤±è´¥: {str(e)}")
```

## âœ… éªŒè¯æ­¥éª¤

### 1. æµ‹è¯•æ–‡æ¡£è§£æ
```python
# æµ‹è¯•è„šæœ¬
import asyncio
from app.services.document_parser import DocumentParserService
from app.models.knowledge import DocumentType

async def test_parser():
    parser = DocumentParserService()
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    with open("test.txt", "w") as f:
        f.write("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ã€‚\n\nåŒ…å«å¤šä¸ªæ®µè½ã€‚")
    
    result = await parser.parse_document("test.txt", DocumentType.TXT)
    print("è§£æç»“æœ:", result)

asyncio.run(test_parser())
```

### 2. æµ‹è¯•å‘é‡åŒ–æœåŠ¡
```python
# æµ‹è¯•å‘é‡åŒ–
import asyncio
from app.services.vector_service import VectorService

async def test_vector():
    vector_service = VectorService()
    
    # æµ‹è¯•å‘é‡åŒ–
    chunks = ["è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬", "è¿™æ˜¯ç¬¬äºŒæ®µæ–‡æœ¬"]
    vector_ids = await vector_service.vectorize_document("test-doc", chunks)
    print("å‘é‡ID:", vector_ids)
    
    # æµ‹è¯•æœç´¢
    results = await vector_service.search_similar("æ–‡æœ¬")
    print("æœç´¢ç»“æœ:", results)

asyncio.run(test_vector())
```

### 3. æµ‹è¯•å®Œæ•´æµç¨‹
```bash
# ä¸Šä¼ æ–‡æ¡£æµ‹è¯•
curl -X POST "http://localhost:8000/api/v1/knowledge/bases/{kb_id}/documents/upload" \
     -H "Content-Type: multipart/form-data" \
     -F "file=@test.txt" \
     -F "doc_type=txt"
```

## ğŸš¨ å¸¸è§é—®é¢˜

### ä¾èµ–å®‰è£…é—®é¢˜
```bash
# å¦‚æœsentence-transformerså®‰è£…å¤±è´¥
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install sentence-transformers
```

### ChromaDBè¿æ¥é—®é¢˜
```python
# æ£€æŸ¥ChromaDBæ˜¯å¦æ­£å¸¸å·¥ä½œ
import chromadb
client = chromadb.Client()
print("ChromaDBè¿æ¥æˆåŠŸ")
```

### æ–‡ä»¶è·¯å¾„é—®é¢˜
```python
# ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
import os
os.makedirs("data/uploads", exist_ok=True)
```

## â¡ï¸ ä¸‹ä¸€æ­¥
æ–‡æ¡£å¤„ç†æœåŠ¡å®Œæˆåï¼Œç»§ç»­ [ç®¡ç†ç•Œé¢](./ç®¡ç†ç•Œé¢.md)